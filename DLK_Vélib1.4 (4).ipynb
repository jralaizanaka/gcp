{"cells": [{"cell_type": "markdown", "id": "fe458b34-9385-4ca6-9672-f2f41eb516d3", "metadata": {}, "source": "# Libraries\n"}, {"cell_type": "code", "execution_count": 1, "id": "59ae7f5b-fa79-4b6d-9245-5c7c598ce496", "metadata": {}, "outputs": [], "source": "import git\nfrom pyspark.sql.functions import*\nimport subprocess\nimport os\nimport sys\nimport time\nfrom pyspark.sql import SparkSession"}, {"cell_type": "markdown", "id": "85d83728-601e-44a6-85f7-ffd971a61498", "metadata": {}, "source": "# Variables Globales"}, {"cell_type": "code", "execution_count": 2, "id": "e15baf84-d472-4299-815a-9aace905b566", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "23/05/04 11:03:19 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"}], "source": "path_to_repo=\"velib_dlk/datalake/\"\nhdfs_repo_name=\"datalake\"\nhive_db_name=\"myVelib\" # Ne pas oublier de cr\u00e9er la db via le terminal\ntable_name=\"my_velib_table_partioned\"\n\n#D\u00e9marrage d'une cession spark\nspark = SparkSession.builder.appName(\"velib\").enableHiveSupport().getOrCreate()\nspark.conf.set(\"spark.sql.repl.eagerEval.enabled\",True)\nspark.conf.set(\"spark.sql.sources.partitionOverwriteMode\",\"dynamic\")"}, {"cell_type": "code", "execution_count": 3, "id": "39da8498-7495-4f22-9c13-03682d9c2202", "metadata": {"tags": []}, "outputs": [], "source": "#! cd /velib_dlk/datalake && git pull"}, {"cell_type": "markdown", "id": "6a87966b-905c-4ff0-82cf-abd4f5c060c3", "metadata": {}, "source": "# Functions"}, {"cell_type": "code", "execution_count": 4, "id": "658a7013-bc32-4f08-8ac6-5f2be7be94dd", "metadata": {}, "outputs": [], "source": "def pull_data(path_to_repo):\n    result_git=subprocess.run(f\"cd /{path_to_repo} && git pull\", shell=True, check=True, capture_output=True, text=True).stdout[:-1]\n    return result_git"}, {"cell_type": "code", "execution_count": 5, "id": "394222f3-bf5a-4629-abac-b8a651cddae7", "metadata": {"tags": []}, "outputs": [], "source": "def put_data_on_hdfs():\n    temps_deb=time.time()\n    !hadoop fs -put -p /velib_dlk/datalake /\n    temps_fin=time.time()\n    return temps_fin-temps_deb"}, {"cell_type": "code", "execution_count": 6, "id": "336c1cc7-9c9e-451b-9d07-49577149c5cd", "metadata": {}, "outputs": [], "source": "def check_hive_table_is_empty(table_name):\n    return spark.sql(f\"SHOW TABLES LIKE '{table_name}'\").count()       "}, {"cell_type": "code", "execution_count": 7, "id": "08f00bb7-597a-4d56-97c5-a53c548d5a1b", "metadata": {}, "outputs": [], "source": "def get_appended_file_names():\n\n    # Open the Git repository\n    repo = git.Repo(path_to_repo)\n\n    # Get the most recent commit\n    commit = repo.head.commit\n\n    # Get the list of changed files in the commit\n    changed_files = commit.stats.files.keys()\n    \n    l=len(changed_files)\n    list_changed_files=[f\"/{hdfs_repo_name}/{list(changed_files)[i]}\" for i in range(l)]\n    return list_changed_files\n"}, {"cell_type": "code", "execution_count": 8, "id": "49fb9a52-476f-42ff-bf10-1f1eb2303098", "metadata": {}, "outputs": [], "source": "def run_cmd(args_list):\n    \"\"\"\n    run linux commands\n    \"\"\"\n    # import subprocess\n    print('Running system command: {0}'.format(' '.join(args_list)))\n    proc = subprocess.Popen(args_list, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    s_output, s_err = proc. communicate()\n    s_return = proc.returncode\n    return s_return, s_output, s_err"}, {"cell_type": "code", "execution_count": 9, "id": "48af769f-257e-4ff3-80cc-c12228e05514", "metadata": {}, "outputs": [], "source": "def get_hadoop_file_paths():\n    table_list=[]\n    (_,out_table, _)= run_cmd(['hdfs', 'dfs', '-ls', '/'+hdfs_repo_name ])\n    for table_name in out_table.decode().split('\\n'):\n        if '.csv' in table_name: \n            table_list.append('/'+ table_name.split(' /')[1])\n    return table_list"}, {"cell_type": "code", "execution_count": 10, "id": "809a9cc2-1061-4656-9489-07618e46013c", "metadata": {}, "outputs": [], "source": "def main():\n    spark.sql(f\"USE {hive_db_name}\")\n    # pull data from repo\n    etat_repo=pull_data(path_to_repo)\n    # put data on HDFS Datalake\n    time_pull_data= put_data_on_hdfs()\n    \n    print (etat_repo)\n    print(f\"temps de chargement hdfs:{time_pull_data} secondes\")\n  \n    \n    if check_hive_table_is_empty(table_name)==0:\n        print(\"cas 1\")\n\n        df_velib = spark.read.csv('/datalake/*.csv', header=True, inferSchema=True)\n    \n        # Add columns for year, month, day, and hour to the DataFrame\n        df_with_partitions = df_velib.withColumn(\"year\", year(col(\"record_timestamp\"))) \\\n                          .withColumn(\"month\", month(col(\"record_timestamp\"))) \\\n                          .withColumn(\"day\", dayofmonth(col(\"record_timestamp\"))) \\\n                          .withColumn(\"dayofweek\", dayofweek(col(\"record_timestamp\"))) \\\n                          .withColumn(\"hour\", hour(col(\"record_timestamp\")))\n\n        # Partition the DataFrame by year, month, day, and hour\n        df_with_partitions.write.mode(\"overwrite\").partitionBy(\"year\", \"month\", \"day\", \"hour\").format(\"parquet\").saveAsTable(table_name)  \n    \n    elif etat_repo==\"Already up to date.\":\n        print(\"up to date\")\n        \n    else:\n        print(\"cas update\")\n        file_paths=get_appended_file_names()\n        print(file_paths)\n        df_velib = spark.read.csv(file_paths, header=True, inferSchema=True)\n        \n        # Add columns for year, month, day, and hour to the DataFrame\n        df_with_partitions = df_velib.withColumn(\"year\", year(col(\"record_timestamp\"))) \\\n                          .withColumn(\"month\", month(col(\"record_timestamp\"))) \\\n                          .withColumn(\"day\", dayofmonth(col(\"record_timestamp\"))) \\\n                          .withColumn(\"dayofweek\", dayofweek(col(\"record_timestamp\"))) \\\n                          .withColumn(\"hour\", hour(col(\"record_timestamp\")))\n        \n        \n        df_with_partitions.write.partitionBy(\"year\", \"month\", \"day\", \"hour\").mode(\"overwrite\").format(\"parquet\").option(\"partitionOverwriteMode\", \"dynamic\").saveAsTable(table_name) \n    \n    nb_lign_hive=spark.sql(f\"SELECT count(*) from {table_name} \")\n    count_int = int(nb_lign_hive.first()[0])\n       \n    print(nb_lign_hive) \n    return count_int"}, {"cell_type": "markdown", "id": "b461f50c-99b0-4a4c-abc1-8dd324b71bb6", "metadata": {}, "source": "# Put data on Hive Datawarehouse"}, {"cell_type": "code", "execution_count": null, "id": "3e7c8174-5eb7-4167-8fe2-72d9e1dd52ce", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used\n"}, {"name": "stdout", "output_type": "stream", "text": "put: `/datalake/27042023_22_33_39.csv': File exists\nput: `/datalake/12042023_12_49_38.csv': File exists\nput: `/datalake/15042023_17_40_19.csv': File exists\nput: `/datalake/10042023_14_29_38.csv': File exists\nput: `/datalake/02052023_14_34_38.csv': File exists\nput: `/datalake/16042023_04_02_49.csv': File exists\nput: `/datalake/06042023_17_42_38.csv': File exists\nput: `/datalake/20042023_08_47_38.csv': File exists\nput: `/datalake/02052023_15_34_39.csv': File exists\nput: `/datalake/25042023_12_25_38.csv': File exists\nput: `/datalake/06042023_15_42_38.csv': File exists\nput: `/datalake/26042023_16_09_38.csv': File exists\nput: `/datalake/11042023_18_20_38.csv': File exists\nput: `/datalake/06042023_12_42_37.csv': File exists\nput: `/datalake/05042023_17_16_38.csv': File exists\nput: `/datalake/06042023_07_26_37.csv': File exists\nput: `/datalake/13042023_02_06_13.csv': File exists\nput: `/datalake/16042023_03_02_09.csv': File exists\nput: `/datalake/30042023_19_08_39.csv': File exists\nput: `/datalake/06042023_11_42_38.csv': File exists\nput: `/datalake/25042023_20_40_38.csv': File exists\nput: `/datalake/15042023_13_01_39.csv': File exists\nput: `/datalake/25042023_13_32_37.csv': File exists\nput: `/datalake/02052023_10_23_39.csv': File exists\nput: `/datalake/09042023_10_44_38.csv': File exists\nput: `/datalake/25042023_16_37_38.csv': File exists\nput: `/datalake/11042023_21_20_38.csv': File exists\nput: `/datalake/20042023_15_02_38.csv': File exists\nput: `/datalake/12042023_13_49_38.csv': File exists\nput: `/datalake/02052023_13_34_38.csv': File exists\nput: `/datalake/07042023_13_12_38.csv': File exists\nput: `/datalake/21042023_11_58_38.csv': File exists\nput: `/datalake/08042023_12_15_39.csv': File exists\nput: `/datalake/11042023_12_11_38.csv': File exists\nput: `/datalake/08042023_13_15_39.csv': File exists\nput: `/datalake/06042023_08_26_37.csv': File exists\nput: `/datalake/27042023_09_07_39.csv': File exists\nput: `/datalake/19042023_09_17_38.csv': File exists\nput: `/datalake/10042023_09_18_39.csv': File exists\nput: `/datalake/06042023_14_42_38.csv': File exists\nput: `/datalake/02052023_09_23_40.csv': File exists\nput: `/datalake/12042023_20_47_38.csv': File exists\nput: `/datalake/20042023_21_44_38.csv': File exists\nput: `/datalake/02052023_16_34_38.csv': File exists\nput: `/datalake/19042023_18_11_38.csv': File exists\nput: `/datalake/05042023_15_14_37.csv': File exists\nput: `/datalake/16042023_12_41_06.csv': File exists\nput: `/datalake/03052023_12_07_39.csv': File exists\nput: `/datalake/06042023_02_25_37.csv': File exists\nput: `/datalake/11042023_19_20_38.csv': File exists\nput: `/datalake/28042023_20_52_38.csv': File exists\nput: `/datalake/09042023_14_44_39.csv': File exists\nput: `/datalake/27042023_14_07_41.csv': File exists\nput: `/datalake/21042023_10_58_38.csv': File exists\nput: `/datalake/08042023_09_13_39.csv': File exists\nput: `/datalake/08042023_20_15_39.csv': File exists\nput: `/datalake/07042023_11_12_39.csv': File exists\nput: `/datalake/17042023_16_31_37.csv': File exists\nput: `/datalake/09042023_18_44_39.csv': File exists\nput: `/datalake/06042023_18_42_39.csv': File exists\nput: `/datalake/11042023_16_20_38.csv': File exists\nput: `/datalake/18042023_12_58_37.csv': File exists\nput: `/datalake/15042023_12_01_38.csv': File exists\nput: `/datalake/27042023_15_07_38.csv': File exists\nput: `/datalake/18042023_13_58_38.csv': File exists\nput: `/datalake/25042023_09_59_38.csv': File exists\nput: `/datalake/03052023_11_07_39.csv': File exists\nput: `/datalake/05042023_21_24_37.csv': File exists\nput: `/datalake/23042023_16_20_38.csv': File exists\nput: `/datalake/06042023_06_26_37.csv': File exists\nput: `/datalake/12042023_18_46_38.csv': File exists\nput: `/datalake/21042023_08_55_38.csv': File exists\nput: `/datalake/12042023_19_46_38.csv': File exists\nput: `/datalake/20042023_16_04_38.csv': File exists\nput: `/datalake/08042023_10_15_38.csv': File exists\nput: `/datalake/26042023_11_58_38.csv': File exists\nput: `/datalake/28042023_19_52_38.csv': File exists\nput: `/datalake/10042023_10_18_39.csv': File exists\nput: `/datalake/19042023_15_05_38.csv': File exists\nput: `/datalake/05042023_23_24_37.csv': File exists\nput: `/datalake/16042023_06_04_13.csv': File exists\nput: `/datalake/15042023_16_21_56.csv': File exists\nput: `/datalake/12042023_15_49_38.csv': File exists\nput: `/datalake/28042023_09_16_39.csv': File exists\nput: `/datalake/21042023_12_58_38.csv': File exists\nput: `/datalake/12042023_14_49_38.csv': File exists\nput: `/datalake/21042023_16_01_39.csv': File exists\nput: `/datalake/07042023_10_12_39.csv': File exists\nput: `/datalake/30042023_18_07_39.csv': File exists\nput: `/datalake/15042023_20_14_14.csv': File exists\nput: `/datalake/05042023_18_17_37.csv': File exists\nput: `/datalake/17042023_09_39_38.csv': File exists\nput: `/datalake/12042023_10_48_39.csv': File exists\nput: `/datalake/16042023_02_01_29.csv': File exists\nput: `/datalake/02052023_12_34_39.csv': File exists\nput: `/datalake/09042023_19_44_38.csv': File exists\nput: `/datalake/10042023_20_29_38.csv': File exists\nput: `/datalake/28042023_14_30_38.csv': File exists\nput: `/datalake/11042023_17_20_38.csv': File exists\nput: `/datalake/07042023_20_33_38.csv': File exists\nput: `/datalake/03052023_14_58_38.csv': File exists\nput: `/datalake/06042023_19_42_38.csv': File exists\nput: `/datalake/03052023_17_26_38.csv': File exists\nput: `/datalake/28042023_17_52_38.csv': File exists\nput: `/datalake/27042023_20_24_39.csv': File exists\nput: `/datalake/28042023_10_16_38.csv': File exists\nput: `/datalake/16042023_08_57_04.csv': File exists\nput: `/datalake/09042023_17_44_39.csv': File exists\nput: `/datalake/15042023_23_23_18.csv': File exists\nput: `/datalake/27042023_12_07_39.csv': File exists\nput: `/datalake/28042023_12_17_38.csv': File exists\nput: `/datalake/19042023_10_17_38.csv': File exists\nput: `/datalake/12042023_11_48_39.csv': File exists\nput: `/datalake/26042023_10_58_38.csv': File exists\nput: `/datalake/27042023_17_22_38.csv': File exists\nput: `/datalake/10042023_12_29_37.csv': File exists\nput: `/datalake/10042023_16_29_38.csv': File exists\nput: `/datalake/06042023_01_24_37.csv': File exists\nput: `/datalake/07042023_15_12_38.csv': File exists\nput: `/datalake/11042023_09_11_39.csv': File exists\nput: `/datalake/27042023_21_24_39.csv': File exists\nput: `/datalake/08042023_17_15_39.csv': File exists\nput: `/datalake/30042023_21_08_39.csv': File exists\nput: `/datalake/08042023_11_15_39.csv': File exists\nput: `/datalake/07042023_18_33_38.csv': File exists\nput: `/datalake/11042023_10_11_38.csv': File exists\nput: `/datalake/21042023_14_00_38.csv': File exists\nput: `/datalake/17042023_13_31_37.csv': File exists\nput: `/datalake/08042023_16_15_39.csv': File exists\nput: `/datalake/08042023_15_15_39.csv': File exists\nput: `/datalake/04052023_10_40_38.csv': File exists\nput: `/datalake/28042023_16_52_38.csv': File exists\nput: `/datalake/07042023_16_12_38.csv': File exists\nput: `/datalake/27042023_18_24_41.csv': File exists\nput: `/datalake/11042023_20_20_38.csv': File exists\nput: `/datalake/30042023_20_08_39.csv': File exists\nput: `/datalake/16042023_07_04_55.csv': File exists\nput: `/datalake/25042023_19_40_38.csv': File exists\nput: `/datalake/18042023_15_00_39.csv': File exists\nput: `/datalake/26042023_12_58_38.csv': File exists\nput: `/datalake/10042023_15_29_38.csv': File exists\nput: `/datalake/27042023_16_16_38.csv': File exists\nput: `/datalake/21042023_09_57_38.csv': File exists\nput: `/datalake/17042023_17_31_37.csv': File exists\nput: `/datalake/10042023_11_18_39.csv': File exists\nput: `/datalake/27042023_10_07_39.csv': File exists\nput: `/datalake/07042023_17_12_39.csv': File exists\nput: `/datalake/28042023_13_17_38.csv': File exists\nput: `/datalake/09042023_16_44_39.csv': File exists\nput: `/datalake/19042023_19_11_38.csv': File exists\nput: `/datalake/09042023_20_44_38.csv': File exists\nput: `/datalake/08042023_21_15_39.csv': File exists\nput: `/datalake/19042023_13_05_38.csv': File exists\nput: `/datalake/03052023_10_07_38.csv': File exists\nput: `/datalake/07042023_09_11_38.csv': File exists\nput: `/datalake/07042023_14_12_39.csv': File exists\nput: `/datalake/09042023_08_44_39.csv': File exists\nput: `/datalake/06042023_16_42_39.csv': File exists\nput: `/datalake/28042023_18_52_38.csv': File exists\nput: `/datalake/25042023_17_38_38.csv': File exists\nput: `/datalake/20042023_19_32_38.csv': File exists\nput: `/datalake/10042023_17_29_38.csv': File exists\nput: `/datalake/12042023_17_46_38.csv': File exists\nput: `/datalake/25042023_08_59_38.csv': File exists\nput: `/datalake/10042023_13_29_38.csv': File exists\nput: `/datalake/27042023_13_07_38.csv': File exists\nput: `/datalake/20042023_13_51_38.csv': File exists\nput: `/datalake/15042023_13_21_59.csv': File exists\nput: `/datalake/09042023_15_44_38.csv': File exists\nput: `/datalake/09042023_12_44_39.csv': File exists\nput: `/datalake/16042023_17_37_10.csv': File exists\nput: `/datalake/20042023_09_47_38.csv': File exists\nput: `/datalake/05042023_14_06_37.csv': File exists\nput: `/datalake/07042023_08_11_38.csv': File exists\nput: `/datalake/03052023_19_27_38.csv': File exists\nput: `/datalake/08042023_14_15_38.csv': File exists\nput: `/datalake/11042023_14_11_38.csv': File exists\nput: `/datalake/20042023_12_51_38.csv': File exists\nput: `/datalake/20042023_18_20_38.csv': File exists\nput: `/datalake/24042023_14_26_38.csv': File exists\nput: `/datalake/18042023_11_58_38.csv': File exists\nput: `/datalake/05042023_20_22_38.csv': File exists\nput: `/datalake/09042023_09_44_38.csv': File exists\nput: `/datalake/11042023_13_11_38.csv': File exists\nput: `/datalake/09042023_13_44_39.csv': File exists\nput: `/datalake/06042023_13_42_37.csv': File exists\nput: `/datalake/02052023_18_35_38.csv': File exists\nput: `/datalake/06042023_04_26_37.csv': File exists\nput: `/datalake/21042023_17_04_39.csv': File exists\nput: `/datalake/30042023_17_07_39.csv': File exists\nput: `/datalake/08042023_19_15_39.csv': File exists\nput: `/datalake/06042023_10_41_37.csv': File exists\nput: `/datalake/20042023_10_49_40.csv': File exists\nput: `/datalake/06042023_20_42_38.csv': File exists\nput: `/datalake/28042023_11_17_38.csv': File exists\nput: `/datalake/19042023_14_05_38.csv': File exists\nput: `/datalake/05042023_22_24_37.csv': File exists\nput: `/datalake/18042023_10_15_38.csv': File exists\nput: `/datalake/17042023_12_31_38.csv': File exists\nput: `/datalake/20042023_11_51_38.csv': File exists\nput: `/datalake/11042023_15_11_38.csv': File exists\nput: `/datalake/26042023_14_58_38.csv': File exists\nput: `/datalake/10042023_19_29_38.csv': File exists\nput: `/datalake/27042023_11_07_39.csv': File exists\nput: `/datalake/05042023_19_18_37.csv': File exists\nput: `/datalake/08042023_18_15_39.csv': File exists\nput: `/datalake/16042023_05_03_30.csv': File exists\nput: `/datalake/16042023_15_12_26.csv': File exists\nput: `/datalake/26042023_13_58_38.csv': File exists\nput: `/datalake/17042023_15_31_37.csv': File exists\nput: `/datalake/19042023_17_11_38.csv': File exists\nput: `/datalake/19042023_16_07_38.csv': File exists\nput: `/datalake/.git/packed-refs': File exists\nput: `/datalake/.git/FETCH_HEAD': File exists\nput: `/datalake/.git/index': File exists\nput: `/datalake/.git/config': File exists\nput: `/datalake/.git/hooks/applypatch-msg.sample': File exists\nput: `/datalake/.git/hooks/fsmonitor-watchman.sample': File exists\nput: `/datalake/.git/hooks/pre-commit.sample': File exists\nput: `/datalake/.git/hooks/pre-applypatch.sample': File exists\nput: `/datalake/.git/hooks/update.sample': File exists\nput: `/datalake/.git/hooks/pre-push.sample': File exists\nput: `/datalake/.git/hooks/pre-receive.sample': File exists\nput: `/datalake/.git/hooks/pre-rebase.sample': File exists\nput: `/datalake/.git/hooks/post-update.sample': File exists\nput: `/datalake/.git/hooks/prepare-commit-msg.sample': File exists\nput: `/datalake/.git/hooks/pre-merge-commit.sample': File exists\nput: `/datalake/.git/hooks/commit-msg.sample': File exists\nput: `/datalake/.git/logs/refs/heads/main': File exists\nput: `/datalake/.git/logs/refs/remotes/origin/main': File exists\nput: `/datalake/.git/logs/refs/remotes/origin/HEAD': File exists\nput: `/datalake/.git/logs/HEAD': File exists\nput: `/datalake/.git/ORIG_HEAD': File exists\nput: `/datalake/.git/refs/heads/main': File exists\nput: `/datalake/.git/refs/remotes/origin/main': File exists\nput: `/datalake/.git/refs/remotes/origin/HEAD': File exists\nput: `/datalake/.git/description': File exists\nput: `/datalake/.git/objects/a6/8953f6209e1e0f4f608b3376534d684e867684': File exists\nput: `/datalake/.git/objects/77/48cb4f99a63bdfad62de3a8b793d54666b913a': File exists\nput: `/datalake/.git/objects/ba/74a4227008a7265a9c6e904bf36593641b28c9': File exists\nput: `/datalake/.git/objects/c6/dff85d45645b5619992ee38d126e9511e7c71d': File exists\nput: `/datalake/.git/objects/8a/a858a56f8ec909b77109e904686e0f3be84a9b': File exists\nput: `/datalake/.git/objects/dc/34d6f2fdf5247ca66aa42db72ef6016b287f3a': File exists\nput: `/datalake/.git/objects/b9/1716606244b240a4b66e0784f267ad9fa424a1': File exists\nput: `/datalake/.git/objects/b9/fcdf86a33bcb3b2c93d56d439bd043593775a9': File exists\nput: `/datalake/.git/objects/92/962b9d99972ebc8e1018abd397f8b69cf77f00': File exists\nput: `/datalake/.git/objects/pack/pack-5ec963488367a9f98d12032431c1cb7da02088b6.idx': File exists\nput: `/datalake/.git/objects/pack/pack-5ec963488367a9f98d12032431c1cb7da02088b6.pack': File exists\nput: `/datalake/.git/HEAD': File exists\nput: `/datalake/.git/info/exclude': File exists\nput: `/datalake/17042023_14_31_37.csv': File exists\nput: `/datalake/06042023_21_42_38.csv': File exists\nput: `/datalake/05042023_16_15_37.csv': File exists\nput: `/datalake/06042023_03_26_37.csv': File exists\nput: `/datalake/02052023_17_34_38.csv': File exists\nput: `/datalake/03052023_18_26_38.csv': File exists\nput: `/datalake/19042023_11_17_40.csv': File exists\nput: `/datalake/25042023_18_39_38.csv': File exists\nput: `/datalake/11042023_11_11_38.csv': File exists\nput: `/datalake/09042023_11_44_39.csv': File exists\nput: `/datalake/20042023_17_15_38.csv': File exists\nput: `/datalake/17042023_18_34_37.csv': File exists\nput: `/datalake/27042023_19_24_39.csv': File exists\nput: `/datalake/10042023_21_29_38.csv': File exists\nput: `/datalake/02052023_19_47_38.csv': File exists\nput: `/datalake/07042023_12_12_39.csv': File exists\nput: `/datalake/20042023_20_44_38.csv': File exists\nput: `/datalake/06042023_00_24_37.csv': File exists\nput: `/datalake/06042023_05_26_37.csv': File exists\nput: `/datalake/07042023_19_33_38.csv': File exists\nput: `/datalake/18042023_09_16_01.csv': File exists\nput: `/datalake/02052023_11_34_38.csv': File exists\nput: `/datalake/15042023_19_10_42.csv': File exists\nput: `/datalake/25042023_14_45_38.csv': File exists\nput: `/datalake/24042023_16_53_38.csv': File exists\nput: `/datalake/16042023_13_42_24.csv': File exists\nput: `/datalake/21042023_15_00_39.csv': File exists\nput: `/datalake/03052023_15_59_39.csv': File exists\nput: `/datalake/10042023_18_29_38.csv': File exists\nput: `/datalake/17042023_10_39_38.csv': File exists\nAlready up to date.\ntemps de chargement hdfs:4.210684299468994 secondes\n"}, {"name": "stderr", "output_type": "stream", "text": "23/05/04 11:03:43 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/05/04 11:03:58 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/05/04 11:04:13 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/05/04 11:04:28 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/05/04 11:04:43 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/05/04 11:04:58 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/05/04 11:05:13 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/05/04 11:05:28 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/05/04 11:05:43 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/05/04 11:05:58 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/05/04 11:06:13 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/05/04 11:06:28 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/05/04 11:06:43 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/05/04 11:06:58 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/05/04 11:07:13 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/05/04 11:07:28 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/05/04 11:07:43 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/05/04 11:07:58 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/05/04 11:08:13 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/05/04 11:08:28 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/05/04 11:08:43 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/05/04 11:08:58 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/05/04 11:09:13 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/05/04 11:09:28 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/05/04 11:09:43 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/05/04 11:09:58 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/05/04 11:10:13 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/05/04 11:10:28 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/05/04 11:10:43 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/05/04 11:10:58 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/05/04 11:11:13 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/05/04 11:11:28 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/05/04 11:11:43 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/05/04 11:11:58 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/05/04 11:12:13 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/05/04 11:12:28 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/05/04 11:12:43 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/05/04 11:12:58 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/05/04 11:13:13 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/05/04 11:13:28 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/05/04 11:13:43 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"}], "source": "nb_lign_hive= main()"}, {"cell_type": "code", "execution_count": null, "id": "475003b7-ed96-4d80-9981-39b22827bdc7", "metadata": {}, "outputs": [], "source": "df_velib_hdfs = spark.read.csv('/datalake/*.csv', header=True, inferSchema=True)\nnb_lign_hdfs= df_velib_hdfs.count()\nprint(f\"nb ligne hdfs:{nb_lign_hdfs}\")"}, {"cell_type": "code", "execution_count": null, "id": "9175355c-114c-41ef-908f-960a15d32edb", "metadata": {}, "outputs": [], "source": "#hive 17146240\n#hdfs 17204640\n#-63320\nres=nb_lign_hive-nb_lign_hdfs\nprint(f\"diff\u00e9rence = {res}\")"}, {"cell_type": "code", "execution_count": null, "id": "61ad16d5-12c6-467c-8809-f3fe484f0dca", "metadata": {}, "outputs": [], "source": "#df_velib_local = spark.read.csv('file:///velib_dlk/datalake/*.csv', header=True, inferSchema=True)\n#df_velib_local.count()"}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.8"}}, "nbformat": 4, "nbformat_minor": 5}